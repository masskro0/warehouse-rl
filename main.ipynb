{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in a Smart Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mdptoolbox import mdp\n",
    "from itertools import product\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load training set...:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store red',\n",
       " 'store red',\n",
       " 'store red',\n",
       " 'store white',\n",
       " 'restore red',\n",
       " 'restore red',\n",
       " 'restore red',\n",
       " 'restore white',\n",
       " 'store blue',\n",
       " 'store blue']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = []\n",
    "with open(\"./data/warehousetraining2x2.txt\") as f:\n",
    "    for line in f:\n",
    "        content = line.splitlines()[0].split(\"\\t\")            # Get rid of any special characters.\n",
    "        training_set.append(content[0] + \" \" + content[1])    # Fuse action and item back together.\n",
    "training_set[0:10]                                            # Display first ten items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **...and test set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store red',\n",
       " 'store white',\n",
       " 'restore white',\n",
       " 'store red',\n",
       " 'store blue',\n",
       " 'store red',\n",
       " 'restore red',\n",
       " 'restore red',\n",
       " 'restore red',\n",
       " 'store red']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = []\n",
    "with open(\"./data/warehouseorder2x2.txt\") as f:\n",
    "    for line in f:\n",
    "        content = line.splitlines()[0].split(\"\\t\")\n",
    "        test_set.append(content[0] + \" \" + content[1])\n",
    "test_set[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot distribution of training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counted = Counter(training_set)                          # Count the occurences of each action.\n",
    "plt.bar(counted.keys(), counted.values(), width=0.45)    # Bar plot.\n",
    "plt.xticks(rotation=45)                                  # Rotate the labels on the x-axis.\n",
    "plt.show()                                               # Show the bar plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define dictionaries for color and action for a consistent assignment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = {\"empty\": 0, \"white\": 1, \"blue\": 2, \"red\": 3}\n",
    "action_dict = {\"store white\": 0, \"restore white\": 1, \"store blue\": 2,\n",
    "             \"restore blue\": 3, \"store red\": 4, \"restore red\": 5}\n",
    "action_reverse_dict = {0: \"store white\", 1: \"restore white\", 2: \"store blue\",\n",
    "                      3: \"restore blue\", 4: \"store red\", 5: \"restore red\"}\n",
    "reward_dict = {0: 10, 1: 5, 2: 5, 3: 2.5, \"error\": -25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 4)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environmental parameters, you can change these.\n",
    "warehouse_shape = (2, 2)    # Shape of warehouse.\n",
    "num_items = 4               # {red, white, blue, empty}\n",
    "num_actions = 6             # {store white, restore white,...}\n",
    "\n",
    "num_cells = np.prod(warehouse_shape)\n",
    "num_states = num_items ** num_cells * num_actions\n",
    "num_states, num_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define function to generate all possible states:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 2],\n",
       "       [0, 0, 0, 0, 3],\n",
       "       [0, 0, 0, 0, 4],\n",
       "       [0, 0, 0, 0, 5],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 1, 3]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_states(num_items, num_cells, num_states, num_actions):\n",
    "    \"\"\"The first four indices represent whether a specific item (empty, red, white, blue) is available\n",
    "       in this field. The fifth index tells which action should be performed (store red,...)\"\"\"\n",
    "    states = []                                          # List of states.\n",
    "    items = np.arange(num_items)                         # List of items, e.g. [0,1,2,3].\n",
    "    cross = list(product(items, repeat=num_cells))       # States of the warehouse.\n",
    "    for state in cross:\n",
    "        for i in range(num_actions):\n",
    "            states.append(state + (i,))                  # Append actions as additional state.\n",
    "    return np.array(states)\n",
    "    \n",
    "\n",
    "states = get_states(num_items, num_cells, num_states, num_actions)\n",
    "print(len(states))\n",
    "states[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define function to calculate the transition model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(num_cells, num_states):\n",
    "    probs = np.zeros((num_cells, num_states, num_states))        # The resulting probability matrix.\n",
    "    \n",
    "    # Put probabilities for each action in a list, sorted the same way as action_(reverse_)dict.\n",
    "    item_probs = [x for x in range(6)]\n",
    "    item_probs = [action_reverse_dict[x] for x in item_probs]\n",
    "    item_probs = [counted[x] for x in item_probs]\n",
    "    s = sum(item_probs)\n",
    "    item_probs = [x/s for x in item_probs]\n",
    "    for pos in range(num_cells):\n",
    "        for i, state in enumerate(states):\n",
    "            order = action_reverse_dict[state[-1]].split(\" \")    # Split into action and color.\n",
    "            next_state = state.copy()                            # Create a copy just to make sure.\n",
    "            next_state[-1] = 0                                   # Facilitates the assignment of probabilities.\n",
    "            if order[0].startswith(\"restore\"):\n",
    "                if state[pos] == item_dict[order[1]]:\n",
    "                    # We can restore the item.\n",
    "                    next_state[pos] = item_dict[\"empty\"]\n",
    "            else:\n",
    "                if state[pos] == item_dict[\"empty\"]:\n",
    "                    # We can store the item.\n",
    "                    next_state[pos] = item_dict[order[1]]\n",
    "            col_idx = np.where(np.all(states == next_state, axis=1))[0][0]\n",
    "            probs[pos][i][col_idx:col_idx+6] = item_probs        # Assign probabilities.\n",
    "    return probs\n",
    "\n",
    "probs = get_transition_matrix(num_cells, num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards(num_states, num_cells):\n",
    "    rewards = np.zeros((num_states, num_cells))        # The resulting reward matrix.\n",
    "    for i, state in enumerate(states):\n",
    "        # Iterate over all states.\n",
    "        order = action_reverse_dict[state[-1]].split(\" \")\n",
    "        if order[0].startswith(\"restore\"):\n",
    "            for pos in range(num_cells):\n",
    "                # Iterate over all positions\n",
    "                if state[pos] == item_dict[order[1]]:\n",
    "                    # We can restore the item.\n",
    "                    rewards[i][pos] = reward_dict[pos]\n",
    "                else:\n",
    "                    # Restoring the item failed.\n",
    "                    rewards[i][pos] = reward_dict[\"error\"]\n",
    "        else:\n",
    "            for pos in range(num_cells):\n",
    "                if state[pos] == item_dict[\"empty\"]:\n",
    "                    # We can store the item.\n",
    "                    rewards[i][pos] = reward_dict[pos]\n",
    "                else:\n",
    "                    # Storing the item failed.\n",
    "                    rewards[i][pos] = reward_dict[\"error\"]\n",
    "    return rewards\n",
    "\n",
    "rewards = get_rewards(num_states, num_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "discount = 0.999\n",
    "max_iterations = 10000\n",
    "\n",
    "# Various MDP Algorithms.\n",
    "mdp_policy = mdp.PolicyIteration(probs, rewards, discount, max_iter=max_iterations)\n",
    "mdp_value = mdp.ValueIteration(probs, rewards, discount, max_iter=max_iterations)\n",
    "mdp_policy_modified = mdp.PolicyIterationModified(probs, rewards, discount, max_iter=max_iterations)\n",
    "mdp_q = mdp.QLearning(probs, rewards, discount, n_iter=max_iterations)\n",
    "mdp_relative = mdp.RelativeValueIteration(probs, rewards, discount, max_iter=max_iterations)\n",
    "mdp_gs = mdp.ValueIterationGS(probs, rewards, discount, max_iter=max_iterations)\n",
    "\n",
    "# Run the algorithms.\n",
    "mdp_policy.run()\n",
    "mdp_value.run()\n",
    "mdp_policy_modified.run()\n",
    "mdp_q.run()\n",
    "mdp_relative.run()\n",
    "mdp_gs.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate the models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's define the greedy algorithm, which we will compare our trained models with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(test_set, num_cells):\n",
    "    # Get the distances to each warehouse cell.\n",
    "    indices = np.indices(warehouse_shape)\n",
    "    distances = (indices[0] + indices[1] + 1) * 2\n",
    "    distances = distances.flatten()\n",
    "    \n",
    "    # Sort the distances and store their indices.\n",
    "    indices = np.argsort(distances)\n",
    "    total_distance = 0\n",
    "    total_reward = 0\n",
    "    current_state = np.zeros(num_cells)\n",
    "    for order in test_set:\n",
    "        # Iterate over test set.\n",
    "        order_split = order.split(\" \")\n",
    "        done = False\n",
    "        if order.startswith(\"store\"):\n",
    "            for idx in indices:\n",
    "                # Iterate over cells, starting from the closest to the farthest ones.\n",
    "                if current_state[idx] == item_dict[\"empty\"]:\n",
    "                    # We can store the item.\n",
    "                    current_state[idx] = item_dict[order_split[1]]    # Update current state.\n",
    "                    total_distance += distances[idx]                  # Update cumulative distance.\n",
    "                    total_reward += reward_dict[idx]                  # Update total reward.\n",
    "                    done = True                                       # Flag for punishing the agent.\n",
    "                    break\n",
    "        else:\n",
    "            for idx in indices:\n",
    "                if current_state[idx] == item_dict[order_split[1]]:\n",
    "                    # We can restore the item.\n",
    "                    current_state[idx] = item_dict[\"empty\"]\n",
    "                    total_distance += distances[idx]\n",
    "                    total_reward += reward_dict[idx]\n",
    "                    done = True\n",
    "                    break\n",
    "        if not done:\n",
    "            # Punish agent for dropping the order.\n",
    "            total_reward += reward_dict[\"error\"]\n",
    "    return total_distance, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's define a function to evaluate our trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_set, states, num_cells):\n",
    "    # Get the distances to each warehouse cell.\n",
    "    indices = np.indices(warehouse_shape)\n",
    "    distances = (indices[0] + indices[1] + 1) * 2\n",
    "    distances = distances.flatten()\n",
    "    total_distance = 0\n",
    "    total_reward = 0\n",
    "    current_state = np.zeros(num_cells + 1)\n",
    "    for order in test_set:\n",
    "        # Iterate over test set.\n",
    "        order_split = order.split(\" \")\n",
    "        current_state[-1] = action_dict[order]\n",
    "        idx = np.where(np.all(states == current_state, axis=1))[0][0]    # Get index in states matrix.\n",
    "        pos = model.policy[idx]                                          # Let the model make a prediction.\n",
    "        total_distance += distances[pos]                                 # Add prediction to total distances.\n",
    "        if order_split[0].startswith(\"restore\"):\n",
    "            if current_state[pos] == item_dict[order_split[1]]:\n",
    "                # We can restore the item.\n",
    "                current_state[pos] = item_dict[\"empty\"]                  # Update current position.\n",
    "                total_reward += reward_dict[pos]                         # Update total reward.\n",
    "            else:\n",
    "                # Punish the agent for not restoring the item.\n",
    "                total_reward += reward_dict[\"error\"]\n",
    "        else:\n",
    "            if current_state[pos] == item_dict[\"empty\"]:\n",
    "                # We can store the item.\n",
    "                current_state[pos] = item_dict[order_split[1]]\n",
    "                total_reward += reward_dict[pos]\n",
    "            else:\n",
    "                # Punish the agent for not storing the item.\n",
    "                total_reward += reward_dict[\"error\"]\n",
    "    return total_distance, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function for printing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(distances, rewards, algorithms, test_set):\n",
    "    for i in range(len(distances)):\n",
    "        print(\"\\n\")\n",
    "        print(\"==================={}===================\".format(algorithms[i]))\n",
    "        print(\"Total driven distance: {}\".format(distances[i]))\n",
    "        print(\"Total rewards: {}\".format(rewards[i]))\n",
    "        print(\"Test set size: {}\".format(len(test_set)))\n",
    "        print(\"==================={}===================\".format(\"=\" * len(algorithms[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualize results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================Policy Iteration===================\n",
      "Total driven distance: 258\n",
      "Total rewards: 330.0\n",
      "Test set size: 65\n",
      "======================================================\n",
      "\n",
      "\n",
      "===================Value Iteration===================\n",
      "Total driven distance: 258\n",
      "Total rewards: 330.0\n",
      "Test set size: 65\n",
      "=====================================================\n",
      "\n",
      "\n",
      "===================Modified Policy Iteration===================\n",
      "Total driven distance: 258\n",
      "Total rewards: 330.0\n",
      "Test set size: 65\n",
      "===============================================================\n",
      "\n",
      "\n",
      "===================Q Learning===================\n",
      "Total driven distance: 236\n",
      "Total rewards: 320.0\n",
      "Test set size: 65\n",
      "================================================\n",
      "\n",
      "\n",
      "===================Relative Value Iteration===================\n",
      "Total driven distance: 258\n",
      "Total rewards: 330.0\n",
      "Test set size: 65\n",
      "==============================================================\n",
      "\n",
      "\n",
      "===================Gauss-Seidel Value Iteration===================\n",
      "Total driven distance: 258\n",
      "Total rewards: 330.0\n",
      "Test set size: 65\n",
      "==================================================================\n",
      "\n",
      "\n",
      "===================Greedy Algorithm===================\n",
      "Total driven distance: 228\n",
      "Total rewards: 427.5\n",
      "Test set size: 65\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Run and gather statistics for each algorithm.\n",
    "policy_results = eval_model(mdp_policy, test_set, states, num_cells)\n",
    "value_results = eval_model(mdp_value, test_set, states, num_cells)\n",
    "policy_modified_results = eval_model(mdp_policy_modified, test_set, states, num_cells)\n",
    "q_learning_results = eval_model(mdp_q, test_set, states, num_cells)\n",
    "relative_value_results = eval_model(mdp_relative, test_set, states, num_cells)\n",
    "value_gs_results = eval_model(mdp_gs, test_set, states, num_cells)\n",
    "greedy_results = greedy(test_set, num_cells)\n",
    "\n",
    "# Print results.\n",
    "distances = [policy_results[0], value_results[0], policy_modified_results[0], q_learning_results[0],\n",
    "             relative_value_results[0], value_gs_results[0], greedy_results[0]]\n",
    "rewards = [policy_results[1], value_results[1], policy_modified_results[1], q_learning_results[1],\n",
    "             relative_value_results[1], value_gs_results[1], greedy_results[1]]\n",
    "algorithms = [\"Policy Iteration\", \"Value Iteration\", \"Modified Policy Iteration\", \"Q Learning\",\n",
    "              \"Relative Value Iteration\", \"Gauss-Seidel Value Iteration\", \"Greedy Algorithm\"]\n",
    "print_results(distances, rewards, algorithms, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
